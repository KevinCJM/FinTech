{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1. Linear Regression 线性回归\n",
    "回归（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。\n",
    "\n",
    "在机器学习领域中的大多数任务通常都与预测（prediction）有关。 当我们想预测一个数值时，就会涉及到回归问题。 常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、 预测需求（零售销量等）。 但不是所有的预测都是回归问题。 在后面的章节中，我们将介绍分类问题。分类问题的目标是预测数据属于一组类别中的哪一个。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97c8566531e0c24f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.1. Basics 线性回归的基本元素\n",
    "线性回归（linear regression）可以追溯到19世纪初， 它在回归的各种标准工具中最简单而且最流行。 线性回归基于几个简单的假设： 首先，假设自变量x和因变量y和之间的关系是线性的， 即y可以表示为x中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。\n",
    "\n",
    "为了解释线性回归，我们举一个实际的例子： 我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。 为了开发一个能预测房价的模型，我们需要收集一个真实的数据集。 这个数据集包括了房屋的销售价格、面积和房龄。 在机器学习的术语中，该数据集称为训练数据集（training data set） 或训练集（training set）。 每行数据（比如一次房屋交易相对应的数据）称为样本（sample）， 也可以称为数据点（data point）或数据样本（data instance）。 我们把试图预测的目标（比如预测房屋价格）称为标签（label）或目标（target）。 预测所依据的自变量（面积和房龄）称为特征（feature）或协变量（covariate）。\n",
    "\n",
    "通常，我们使用n来表示数据集中的样本数量。 对索引为i的样本，其输入表示为$\\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}]$，其对应的标签是$y^{(i)}$。\n",
    "解释公式\n",
    "- $n$：数据集中的样本数量。\n",
    "- $\\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}]$：索引为 $i$ 的样本的输入特征向量，其中 $x_1^{(i)$ 表示第 $i$ 个样本的面积，$x_2^{(i)}$ 表示第 $i$ 个样本的房龄。\n",
    "- $y^{(i)}$：索引为 $i$ 的样本的标签，即第 $i$ 个样本的房价。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "280ed336186889cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**线性模型**\n",
    "线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和，如下面的式子：\n",
    "$$price = w_{area} \\times area + w_{age} \\times age + b$$\n",
    "其中的$w_{area}$、$w_{age}$称为权重（weight），权重决定了每个特征对我们预测值的影响。 $b$称为偏置（bias）、偏移量（offset）或截距（intercept）。 偏置是指当所有特征都取值为0时，预测值应该为多少。 即使现实中不会有任何房子的面积是0或房龄正好是0年，我们仍然需要偏置项。 如果没有偏置项，我们模型的表达能力将受到限制。 严格来说, 上面公式是输入特征的一个 仿射变换（affine transformation）。 仿射变换的特点是通过加权和对特征进行线性变换（linear transformation）， 并通过偏置项来进行平移（translation）。(上面公式中简化掉了残差项 $\\epsilon$)  \n",
    "给定一个数据集，我们的目标是寻找模型的权重$\\mathbf{x}$和偏置$b$，使得根据模型做出的预测大体符合数据里的真实价格。 输出的预测值由输入特征通过线性模型的仿射变换决定，仿射变换由所选权重和偏置确定。\n",
    "而在机器学习领域，我们通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。 当我们的输入包含$d$个特征时，我们将预测结果$\\hat{y}$（通常使用“尖角”符号表示$y$的估计值）表示为：\n",
    "$$\\hat{y}= w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\ldots + w_d \\cdot x_d + b$$\n",
    "将所有特征放到向量$\\mathbf{x} \\in \\mathbb{R}^d$中，并将所有权重放到向量$\\mathbf{w} \\in \\mathbb{R}^d$中，我们可以用点积形式来简洁地表达模型：$$\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b$$\n",
    "在上面公式中, 向量$\\mathbf{x}$对应于单个数据样本的特征。 用符号表示的矩阵$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$可以很方便地引用我们整个数据集的$n$个样本。 其中，$\\mathbf{X}$的每一行是一个样本，每一列是一种特征。\n",
    "对于特征集合$\\mathbf{X}$，预测值$\\hat{\\mathbf{y}} \\in  \\mathbb{R}^n$可以通过矩阵-向量乘法表示为：\n",
    "$$\\hat{\\mathbf{y}} = \\mathbf{X} \\mathbf{w} + b$$\n",
    "\n",
    "这个过程中的求和将使用numpy的广播机制。 给定训练数据特征$\\mathbf{X}$和对应的已知标签$\\mathbf{y}$，线性回归的目标是找到一组权重向量$\\mathbf{w}$和偏置$b$，尽可能的使得预测值$\\hat{\\mathbf{y}} \\approx \\mathbf{y}$。 当给定从$\\mathbf{X}$的同分布中取样的新样本特征时， 这组权重向量和偏置能够使得新样本预测标签的误差尽可能小。\n",
    "虽然我们相信/假定给定$\\mathbf{x}$和预测$y$的最佳模型会是线性的， 但我们很难找到一个有$n$个样本的真实数据集，其中对于所有的$1\\leq i \\le n$，$y_{(i)}$完全等于$\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b$。 因此，即使我们找到了最佳的$\\mathbf{w}$和$b$，我们仍然会观察到少量的观测误差。 因此，即使确信特征与标签的潜在关系是线性的， 我们也会加入一个噪声项来考虑观测误差带来的影响。 在统计学中，这个噪声通常被假定为均值为0的正态分布。 我们假设噪声$\\epsilon$与特征$x$是独立的，并且$\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$。 因此，我们可以将观测$y$建模为：$$\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b + \\epsilon \\text{  或  }\n",
    "\\hat{\\mathbf{y}} = \\mathbf{X} \\mathbf{w} + b + \\epsilon$$\n",
    "\n",
    "在开始寻找最好的模型参数（model parameters）$\\mathbf{w}$和$b$之前，我们需要两个东西：\n",
    "（1）一个模型评估指标，用来衡量模型的好坏 (模型质量的度量方式)\n",
    "（2）一种能够更新模型以提高模型预测质量的方法"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "583aa4e0c5205583"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Loss Function 损失函数**\n",
    "\n",
    "在我们开始考虑如何用模型拟合（fit）数据之前，我们需要确定一个拟合程度的度量。 损失函数（loss function）能够量化目标的实际值与预测值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 回归问题中最常用的损失函数是平方误差函数。 当样本 $i$ 的预测值为 $\\hat{y}^{(i)}$ 时，平方误差可以定义为以下公式：\n",
    "$$\n",
    "l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2\n",
    "$$\n",
    "常数$1/2$不会带来本质的差别，但这样在形式上稍微简单一些 （因为当我们对损失函数求导后常数系数为1）。 由于训练数据集并不受我们控制，所以经验误差只是关于模型参数的函数。 为了进一步说明，来看下面的例子。 我们为一维情况下的回归问题绘制图像\n",
    "![avatar](/Users/chenjunming/Desktop/FinTech/DeepLearning/pics/WX20240617-191020@2x.png#pic_center)\n",
    "由于均方误差函数中的二次方项， 估计值$\\hat{y}^{(i)}$ 和真实值$y^{(i)}$ 之间较大的差异将贡献更大的损失。 为了衡量整个数据集上的模型预测质量，我们可以计算在$n$个样本的训练集上的平均损失（也等价于求和）。\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\mathbf{w}, b) & = \\frac{1}{n} \\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) \\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2 \\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2} \\left( \\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)} \\right)^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "在训练模型时，我们希望寻找一组参数$(\\mathbf{w}^*, b^*)$，这组参数能够最小化在所有训练样本上的总损失。 换句话说，我们的目标是求解最小化均方误差 (MSE)：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{\\mathbf{w}, b} \\quad & \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2} \\left( \\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)} \\right)^2 \\\\\n",
    "\\text{subject to} \\quad & \\mathbf{w} \\in \\mathbb{R}^d, b \\in \\mathbb{R}\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76a550d04c9487ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Analytic Solution 解析解**\n",
    "\n",
    "线性回归刚好是一个很简单的优化问题。 与我们将在本书中所讲到的其他大部分模型不同，线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解（analytical solution）。 首先，我们将偏置$b$合并到参数$\\mathbf{w}$中，合并方法是在包含所有参数的矩阵中附加一列。 我们的预测问题是最小化$\\|\\mathbf{y} - \\mathbf{X} \\mathbf{w}\\|^2$。 这在损失平面上只有一个临界点，这个临界点对应于整个区域的损失极小点。 将损失关于$\\mathbf{w}$的导数设为0，我们得到了解析解：$$\\mathbf{w}^* = \\left( \\mathbf{X}^\\top \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "\n",
    "推导过程为:\n",
    "\n",
    "- 目标, 最小化函数: $L(\\mathbf{w})=\\|\\mathbf{y} - \\mathbf{X} \\mathbf{w}\\|^2$\n",
    "- 假设我们有 $n$ 个样本，每个样本有 $p$ 个特征：\n",
    "  - $\\mathbf{X}$ 是一个 $n \\times (p+1)$ 的矩阵，其中每一行是一个样本的特征向量，并在最后一列加上了一个常数1，以合并偏置项 $b$。\n",
    "  - $\\mathbf{w}$ 是一个 $(p+1) \\times 1$ 的列向量，表示模型的权重。\n",
    "  - $\\mathbf{y}$ 是一个 $n \\times 1$ 的列向量，表示真实值。\n",
    "- 展开目标函数 $$L(\\mathbf{w}) = (\\mathbf{y} - \\mathbf{X} \\mathbf{w})^\\top (\\mathbf{y} - \\mathbf{X} \\mathbf{w}) \\\\ L(\\mathbf{w}) = \\mathbf{y}^\\top \\mathbf{y} - 2\\mathbf{y}^\\top \\mathbf{X} \\mathbf{w} + \\mathbf{w}^\\top \\mathbf{X}^\\top \\mathbf{X} \\mathbf{w}$$ \n",
    "- 计算梯度, 我们需要找到使损失函数最小化的 $\\mathbf{w}$。为此，我们对 $\\mathbf{w}$ 求导并设导数为零：$$\\nabla_{\\mathbf{w}} L(\\mathbf{w}) = -2\\mathbf{X}^\\top \\mathbf{y} + 2\\mathbf{X}^\\top \\mathbf{X} \\mathbf{w} \\\\ -2\\mathbf{X}^\\top \\mathbf{y} + 2\\mathbf{X}^\\top \\mathbf{X} \\mathbf{w} = 0$$\n",
    "- 解方程: $$\\mathbf{X}^\\top \\mathbf{X} \\mathbf{w} = \\mathbf{X}^\\top \\mathbf{y} \\\\ \\mathbf{w}^* = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "\n",
    "像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。 解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a63d75529d759b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Minibatch Stochastic Gradient Descent 随机梯度下降**\n",
    "\n",
    "即使在我们无法得到解析解的情况下，我们仍然可以有效地训练模型。 在许多任务上，那些难以优化的模型效果要更好。 因此，弄清楚如何训练这些难以优化的模型是非常重要的。\n",
    "\n",
    "这里, 我们用到一种名为梯度下降（gradient descent）的方法， 这种方法几乎可以优化所有深度学习模型。 它通过不断地在损失函数递减的方向上更新参数来降低误差。\n",
    "\n",
    "梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。 但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做小批量随机梯度下降（minibatch stochastic gradient descent）。首先咱们先了解下什么是梯度下降和随机梯度下降:\n",
    "\n",
    "- 梯度下降 (Gradient Descent)\n",
    "梯度下降通过以下步骤来最小化损失函数：\n",
    "    1. 计算梯度：计算损失函数对模型参数的导数（梯度），表示在每个参数方向上的变化率。\n",
    "    2. 更新参数：根据梯度，沿梯度负方向更新模型参数，以减少损失。\n",
    "    3. 梯度下降的基本公式为：$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\nabla_{\\mathbf{w}} L(\\mathbf{w})$\n",
    "    其中：\n",
    "          - $\\mathbf{w}$ 是模型参数。\n",
    "          - $\\eta$ 是学习率（控制每次更新的步长）。\n",
    "          - $\\nabla_{\\mathbf{w}} L(\\mathbf{w})$ 是损失函数 $L(\\mathbf{w})$ 对参数 $\\mathbf{w}$ 的梯度。 \n",
    "    4. 公式解释：\n",
    "          - 计算损失函数 $L(\\mathbf{w})$ 对参数 $\\mathbf{w}$ 的梯度 $\\nabla_{\\mathbf{w}} L(\\mathbf{w})$。\n",
    "          - 用负梯度方向更新参数 $\\mathbf{w}$，即 $\\mathbf{w} - \\eta \\nabla_{\\mathbf{w}} L(\\mathbf{w})$。\n",
    "          - 这个过程会不断重复，直到损失函数收敛到一个最小值或者达到预设的迭代次数。\n",
    "- 随机梯度下降 (Stochastic Gradient Descent, SGD)\n",
    "对于大型数据集，遍历整个数据集（即使用所有样本）来计算梯度和更新参数的开销非常大。因此，引入了随机梯度下降（SGD）. SGD通过每次只使用一个样本来计算梯度并更新参数，从而大幅降低每次更新的计算开销。与标准梯度下降法不同，标准梯度下降法在每次更新时都遍历整个数据集，而随机梯度下降每次只使用一个样本。虽然随机梯度下降的更新方向受单个样本影响较大，导致更新过程波动较大，但它通常能够更快地收敛到一个较好的解。：\n",
    "    1. 单样本更新：在每次参数更新时，仅使用一个样本来计算梯度并更新参数。\n",
    "    2. 更新公式：$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\nabla_{\\mathbf{w}} L^{(i)}(\\mathbf{w})$\n",
    "       其中 $L^{(i)}(\\mathbf{w})$ 是第 $i$ 个样本的损失\n",
    "    3. 虽然 SGD 可以大幅降低每次更新的计算开销，但它的更新方向受单个样本影响较大，更新过程波动较大。\n",
    "\n",
    "小批量随机梯度下降 (Minibatch Stochastic Gradient Descent) 结合了全梯度下降和随机梯度下降的优点,在每次迭代中，我们首先随机抽样一个小批量 $\\mathcal{B}$，它是由固定数量的训练样本组成的。 然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。 最后，我们将梯度乘以一个预先确定的正数$\\eta$（学习率）来更新模型参数, 并从当前参数的值中减掉。\n",
    "\n",
    "我们用下面的数学公式来表示这一更新过程（$\\partial$表示偏导数）：$$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b)$$\n",
    "总结一下，算法的步骤如下： （1）初始化模型参数的值，如随机初始化； （2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。 对于平方损失和仿射变换，我们可以明确地写成如下形式: \n",
    "$$\\begin{aligned} \\mathbf{w} & \\leftarrow \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) && = \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)\\\\ b &\\leftarrow b -  \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\partial_b l^{(i)}(\\mathbf{w}, b) &&  = b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right). \\end{aligned}$$\n",
    "上面公式中的$\\mathcal{w}$和$\\mathcal{x}$都是向量。 在这里，更优雅的向量表示法比系数表示法 (如$w_1,w_2,\\ldots,w_d$) 更具可读性。$|\\mathcal{B}|$表示每个小批量中的样本数，这也称为批量大小（batch size）。$\\eta$表示学习率（learning rate）。 批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。 这些可以调整但不在训练过程中更新的参数称为超参数（hyperparameter）。 调参（hyperparameter tuning）是选择超参数的过程。 超参数通常是我们根据训练迭代结果来调整的， 而训练迭代结果是在独立的验证数据集（validation dataset）上评估得到的。\n",
    "\n",
    "在训练了预先确定的若干迭代次数后（或者直到满足某些其他停止条件后）， 我们记录下模型参数的估计值，表示为$\\hat{\\mathbf{w}}$, $\\hat{b}$。但是，即使我们的函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值。 因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值。\n",
    "\n",
    "线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为泛化（generalization）。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d5c74e49ef0bd93"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f99d44bf50389c21"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9db31f09154137ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
