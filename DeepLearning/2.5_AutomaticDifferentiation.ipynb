{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5. Automatic Differentiation 自动微分\n",
    "在上一节中说道, 求导是几乎所有深度学习优化算法的关键步骤。 虽然求导的计算很简单，只需要一些基本的微积分。 但对于复杂的模型，手工进行更新是一件很痛苦的事情（而且经常容易出错）。\n",
    "深度学习框架通过自动计算导数，即自动微分（automatic differentiation）来加快求导。 实际中，根据设计好的模型，系统会构建一个计算图（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。 这里，反向传播（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。 \n",
    "\n",
    "- 自动微分主要有两种模式：\n",
    "  - 前向模式自动微分（Forward Mode AD）：从输入开始逐步计算到输出。\n",
    "  - 反向模式自动微分（Reverse Mode AD）：从输出开始逐步计算到输入，这种模式更适用于深度学习中的多层网络，因为计算效率更高。\n",
    "\n",
    "- 计算图（Computational Graph）是一个有向无环图（DAG），表示计算过程中的操作和变量。在计算图中：\n",
    "  - 节点（Nodes）表示变量或中间计算结果。\n",
    "  - 边（Edges）表示变量之间的操作。\n",
    "\n",
    "- 通过计算图，我们可以清晰地表示计算的依赖关系，并系统化地追踪计算过程。例如，对于一个简单的函数 $f(x) = x^2+yx$，我们可以画出计算图如下：\n",
    "    $$x \\rightarrow [平方] \\rightarrow x^2 \\\\ y \\rightarrow [乘法] \\rightarrow yx \\\\ [加法] \\rightarrow x^2 + yx$$\n",
    "  \n",
    "- 为什么反向传播梯度重要\n",
    "  - 高效计算：反向传播算法能够高效地计算复杂模型的梯度，避免了手工推导的繁琐和容易出错。\n",
    "  - 优化参数：在深度学习中，梯度用于优化模型参数，通过最小化损失函数来提高模型的性能。反向传播使得我们能够系统化地计算梯度，从而优化模型。\n",
    "  - 自动化流程：自动微分和反向传播允许深度学习框架自动处理梯度计算，使得模型训练和调试变得更加方便和高效。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14e40ee927214057"
  },
  {
   "cell_type": "markdown",
   "source": [
    "为了详细说明计算图如何使用反向传播计算梯度，我们将以 $y = 2\\mathbf{x}^T \\mathbf{x}$ 为例来演示。这个过程包括前向传播（计算函数值）和反向传播（计算梯度）的步骤。我们将逐步构建计算图，并展示如何通过这个图计算梯度。\n",
    "\n",
    "**前向传播**\n",
    "函数: $y = 2\\mathbf{x}^T \\mathbf{x}$; 假设 $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix}$，即 $\\mathbf{x}$ 是一个四维向量。\n",
    "步骤:\n",
    "1. 计算 $\\mathbf{x}^T \\mathbf{x}$：向量的点积。\n",
    "2. 将结果乘以 2。\n",
    "\n",
    "构建计算图如下：\n",
    "```\n",
    "   x_1    x_2    x_3    x_4\n",
    "    |      |      |      |\n",
    "    |      |      |      |\n",
    "  [平方] [平方] [平方] [平方]\n",
    "    |      |      |      |\n",
    "    +------|------|------+\n",
    "           |\n",
    "        [加法]\n",
    "           |\n",
    "           v\n",
    "          S = x_1^2 + x_2^2 + x_3^2 + x_4^2\n",
    "           |\n",
    "        [乘以2]\n",
    "           |\n",
    "           v\n",
    "          y = 2 * S\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "753786d4ca1379fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**反向传播,计算梯度**\n",
    "\n",
    "目标: 计算 $\\nabla_{\\mathbf{x}} y$，即 $y$ 关于 $\\mathbf{x}$ 的梯度。在反向传播过程中，我们从输出节点开始，逐层计算梯度，直到输入节点。具体步骤如下：\n",
    "1. 从最终输出 $y$ 开始, $y = 2S$, \n",
    "这里 $S = \\mathbf{x}^T \\mathbf{x}$\n",
    "所以我们有： $\\frac{\\partial y}{\\partial S} = 2$\n",
    "\n",
    "2. 传播到 $S$: $S = x_1^2 + x_2^2 + x_3^2 + x_4^2$, 我们需要计算 $\\frac{\\partial S}{\\partial x_i}$\n",
    "$\\frac{\\partial S}{\\partial x_i} = 2x_i$   对于每个   $i \\in \\{1, 2, 3, 4\\}$\n",
    "\n",
    "3. 应用链式法则 $\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial y}{\\partial S} \\cdot \\frac{\\partial S}{\\partial x_i}$\n",
    "因此, $\\frac{\\partial y}{\\partial x_i} = 2 \\cdot 2x_i = 4x_i$\n",
    "\n",
    "4. 所以, $\\nabla_{\\mathbf{x}} y = \\begin{pmatrix} 4x_1 \\\\ 4x_2 \\\\ 4x_3 \\\\ 4x_4 \\end{pmatrix}=4\\mathbf{x}$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "665f38ccb15a59ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5.1. A Simple Function 一个简单的例子\n",
    "作为一个演示例子，假设我们想对函数 $y = 2\\mathbf{x}^T \\mathbf{x}$ 关于列向量 $\\mathbf{x}$ 求导。 根据公式$\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}$我们可以很容易地计算导数。 结果应该是 $4\\mathbf{x}$。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c13404b36daf6e7"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 1., 2., 3.])"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import autograd, np, npx\n",
    "\n",
    "npx.set_np()\n",
    "\n",
    "x = np.arange(4.0)  # 创建一个包含 [0.0, 1.0, 2.0, 3.0] 的一维数组（向量）\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:37:39.730333Z",
     "start_time": "2024-06-14T06:37:39.665824Z"
    }
   },
   "id": "3dae76d01cd8f934"
  },
  {
   "cell_type": "markdown",
   "source": [
    "在我们计算$y$关于$\\mathbf{x}$的导数之前，需要一个地方来存储梯度。 重要的是，我们不会在每次对一个参数求导时都分配新的内存。 \n",
    "因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。 \n",
    "注意，一个标量函数关于向量$\\mathbf{x}$的梯度是向量，并且与$\\mathbf{x}$具有相同的形状。在数学中，标量函数（scalar function）是指输入为向量或标量，但输出为单个标量的函数。 这里，函数$y = 2\\mathbf{x}^T \\mathbf{x}$的输入是一个向量$\\mathbf{x}$，输出是一个标量。\n",
    "\n",
    "举例说明, 为什么一个标量函数关于向量$\\mathbf{x}$的梯度是向量，并且与$\\mathbf{x}$具有相同的形状; 考虑标量函数 $f(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{x}$，其中 $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ 是一个二维向量。\n",
    "\n",
    "1. 函数展开：$f(\\mathbf{x}) = x_1^2 + x_2^2$\n",
    "2. 计算偏导数：$\\frac{\\partial f}{\\partial x_1} = 2x_1, \\quad \\frac{\\partial f}{\\partial x_2} = 2x_2$\n",
    "3. 梯度向量：$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\begin{pmatrix} 2x_1 \\\\ 2x_2 \\end{pmatrix}$\n",
    "\n",
    "可见梯度向量 $\\nabla_{\\mathbf{x}} f(\\mathbf{x})$ 的维度与输入向量 $\\mathbf{x}$ 相同，都是二维向量。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "990bf2dab42c41b2"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0., 0., 0.])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过调用attach_grad来为一个张量的梯度分配内存\n",
    "x.attach_grad()  # 将x的梯度分配内存, 且与x形状相同\n",
    "# 在计算关于x的梯度后，将能够通过'grad'属性访问它，它的值被初始化为0\n",
    "x.grad"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:37:39.733781Z",
     "start_time": "2024-06-14T06:37:39.670988Z"
    }
   },
   "id": "4cbd37e86fcf7ade"
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在计算 $y$\n",
    "在代码中，y = 2 * np.dot(x, x) 实际上与数学表达式 $y = 2\\mathbf{x}^T \\mathbf{x}$ 是等价的。 \n",
    "这是因为在 NumPy 中，np.dot(x, x) 对于一维数组（向量）来说，计算的结果是内积（dot product），而这个内积在数学上等价于 $\\mathbf{x}^T \\mathbf{x}$。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd337455439c1db5"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([0., 1., 2., 3.]), array(28.))"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把代码放到autograd.record内，以建立计算图\n",
    "with autograd.record():  # 记录计算图。在这个 with 上下文管理器内部执行的所有操作都会被记录下来，从而允许后续进行反向传播计算梯度。\n",
    "    y = 2 * np.dot(x, x)\n",
    "x, y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:37:39.764718Z",
     "start_time": "2024-06-14T06:37:39.675521Z"
    }
   },
   "id": "fe00ce6f93c64642"
  },
  {
   "cell_type": "markdown",
   "source": [
    "x是一个长度为4的向量，计算x和x的点积，得到了我们赋值给y的标量输出。 接下来，通过调用反向传播函数来自动计算y关于x每个分量的梯度，并打印这些梯度。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb95ad09134b3dcc"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.,  4.,  8., 12.])"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()  # 反向传播计算梯度\n",
    "x.grad"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:37:39.767264Z",
     "start_time": "2024-06-14T06:37:39.687949Z"
    }
   },
   "id": "5c6974b6e4826c05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "函数 $y = 2\\mathbf{x}^T \\mathbf{x}$ 关于列向量 $\\mathbf{x}$ 的梯度应该是 $4\\mathbf{x}$。 让我们快速验证这个梯度是否计算正确。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2466fd7780901916"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ True,  True,  True,  True])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:37:39.777372Z",
     "start_time": "2024-06-14T06:37:39.691124Z"
    }
   },
   "id": "d2f8b4b178475aa8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在计算x的另一个函数。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b69360c3252925e4"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1., 1., 1., 1.])"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x.sum()\n",
    "y.backward()\n",
    "x.grad  # 被新计算的梯度覆盖"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:37:39.778770Z",
     "start_time": "2024-06-14T06:37:39.697631Z"
    }
   },
   "id": "30ff2f86c506f647"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5.2. Backward for Non-Scalar Variables 非标量变量的反向传播\n",
    "当$y$不是标量时，向量$\\mathbf{y}$关于向量$\\mathbf{x}$的导数的最自然解释是一个矩阵。 \n",
    "\n",
    "向量$\\mathbf{y}$关于向量$\\mathbf{x}$的导数: 当输出 $\\mathbf{y}$ 和输入 $\\mathbf{x}$ 都是向量时，计算它们的导数（也称为雅可比矩阵）自然会得到一个矩阵。对于 $\\mathbf{y} \\in \\mathbb{R}^m$ 和 $\\mathbf{x} \\in \\mathbb{R}^n$，雅可比矩阵 $ J$ 的大小是 $ m \\times n$，其元素是 $\\frac{\\partial y_i}{\\partial x_j}$。\n",
    "\n",
    "对于高阶和高维的$\\mathbf{y}$和$\\mathbf{x}$，求导的结果可以是一个高阶张量。例如, 当输入和输出都是二维矩阵时，导数的结果可能会是一个三维张量。对于更高阶和高维的情况，导数可能会变得更加复杂。\n",
    "\n",
    "然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中）， 但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 \n",
    "这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "721909194bf238e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "例子解释\n",
    "- 损失函数与批量处理\n",
    "  - 假设我们有一个损失函数 $L$ 和一批训练样本。对于每个样本 $i$，损失函数为 $L_i$，我们要计算的是批量中所有样本损失的总和的梯度，即：$L_{\\text{batch}} = \\sum_{i} L_i$\n",
    "- 梯度计算\n",
    "  - 我们计算的是总损失函数 $L_{\\text{batch}}$ 对模型参数 $\\mathbf{x}$ 的梯度，而不是每个单独的 $\\frac{\\partial L_i}{\\partial \\mathbf{x}}$。在这种情况下，我们需要计算： \n",
    "    $$\\nabla_{\\mathbf{x}} L_{\\text{batch}} = \\nabla_{\\mathbf{x}} \\left( \\sum_{i} L_i \\right) = \\sum_{i} \\nabla_{\\mathbf{x}} L_i$$\n",
    "- 为什么这样做\n",
    "  - 计算效率：计算一个总梯度比计算雅可比矩阵要简单得多，尤其是在高维情况下。\n",
    "  - 实际需求：在模型训练中，我们关心的是如何调整模型参数以最小化总损失函数，而不是单个样本的损失。\n",
    "  - 内存和计算资源：计算并存储高阶张量或雅可比矩阵需要大量的内存和计算资源。通过只计算总梯度，我们可以节省这些资源。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49ff469e45b545b9"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([0., 1., 2., 3.]), array([0., 1., 4., 9.]))"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x * x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:37:39.778884Z",
     "start_time": "2024-06-14T06:37:39.702614Z"
    }
   },
   "id": "97c60f7e87625120"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 2., 4., 6.])"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当对向量值变量y（关于x的函数）调用backward时，将通过对y中的元素求和来创建一个新的标量变量。然后计算这个标量变量相对于x的梯度\n",
    "with autograd.record():\n",
    "    y = x * x  # y是一个向量\n",
    "y.backward()  # 方向传播\n",
    "x.grad  # 查看梯度, 等价于y=sum(x*x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:37:39.779165Z",
     "start_time": "2024-06-14T06:37:39.707293Z"
    }
   },
   "id": "8bd4cebcb03dc7e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "为了理解如何通过计算图计算梯度，让我们分解上面代码的每个步骤：\n",
    "\n",
    "1. 前向传播：\n",
    "- 计算 $ y = x \\cdot x $, 这里，$y = [x_1^2, x_2^2, x_3^2, x_4^2]$：\n",
    "2. 隐式求和：\n",
    "- 调用 `y.backward()` 时，系统会隐式地将 `y` 中的元素求和：$L = \\sum_{i} y_i = x_1^2 + x_2^2 + x_3^2 + x_4^2$\n",
    "3. 计算梯度：\n",
    "- 反向传播计算标量 $L$ 关于向量 $x$ 的梯度：$\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial (x_1^2 + x_2^2 + x_3^2 + x_4^2)}{\\partial x_i} = 2x_i$\n",
    "因此，梯度向量是：$\\nabla_{\\mathbf{x}} L = \\begin{pmatrix} 2x_1 \\\\ 2x_2 \\\\ 2x_3 \\\\ 2x_4 \\end{pmatrix}$\n",
    "\n",
    "**为什么梯度是向量而不是矩阵**\n",
    "- 当我们调用 `y.backward()` 时，我们在计算一个标量（$L$）关于向量（$\\mathbf{x}$）的导数。结果是一个向量，而不是矩阵。\n",
    "- 这个向量是 $L$ 关于 $\\mathbf{x}$ 的梯度，而不是 $L$ 中每个元素关于 $\\mathbf{x}$ 的梯度。\n",
    "- 如果我们没有对 $\\mathbf{y}$ 的元素求和，而是对每个元素分别计算梯度，那么梯度将是一个矩阵（雅可比矩阵）。但是在深度学习中，我们通常需要的是损失函数（标量）关于输入参数的梯度，这就是为什么最终的梯度是一个与 $\\mathbf{x}$ 形状相同的向量。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11bea31df19ab317"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5.3. Detaching Computation 分离计算\n",
    "有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。 想象一下，我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。\n",
    "这里可以分离y来返回一个新变量u，该变量与y具有相同的值， 但丢弃计算图中如何计算y的任何信息。 换句话说，梯度不会向后流经u到x。 因此，下面的反向传播函数计算z=u*x关于x的偏导数，同时将u作为常数处理， 而不是z=x*x*x关于x的偏导数。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d65fa5302869ea6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "例子解释\n",
    "假设我们有变量 $x$：\n",
    "- $y$ 是 $x$ 的函数：$y = f(x)$\n",
    "- $z$ 是 $y$ 和 $x$ 的函数：$z = g(y, x)$\n",
    "\n",
    "通常情况下，当我们计算 $z$ 关于 $x$ 的梯度时，梯度会通过 $y$ 传播回 $x$，即考虑了 $y$ 作为 $x$ 的函数的影响。然而，有时我们希望将 $y$ 视为常数，不考虑 $y$ 的计算过程对 $x$ 的影响。\n",
    "这可以通过分离计算来实现。通过分离计算，我们创建一个新变量 $u$，它的值等于 $y$，但丢弃了如何计算 $y$ 的任何信息。这样，梯度在反向传播时不会通过 $u$ 向前传播到 $x$。\n",
    "\n",
    "**正常的计算图**\n",
    "\n",
    "1. 前向传播：\n",
    "   $$\n",
    "   y = x^2\n",
    "   $$\n",
    "   $$\n",
    "   z = y \\cdot x = x^2 \\cdot x = x^3\n",
    "   $$\n",
    "\n",
    "2. 反向传播：\n",
    "   - 正常情况下，计算 $z$ 关于 $x$ 的梯度会考虑到 $y$ 的计算过程：\n",
    "     $$\n",
    "     \\frac{dz}{dx} = \\frac{d}{dx}(x^3) = 3x^2\n",
    "     $$\n",
    "\n",
    "**分离计算图**\n",
    "\n",
    "1. 前向传播：\n",
    "   - 创建一个新变量 $u$，使 $u$ 等于 $y$ 的值，但不包含计算 $y$ 的信息：$$u = y.detach() \\quad \\text{（假设这里的 detach 操作分离了计算图）}$$\n",
    "   - 现在计算 $z$：\n",
    "     $$\n",
    "     z = u \\cdot x\n",
    "     $$\n",
    "2. 反向传播：\n",
    "   - 由于 $u$ 是从 $y$ 分离出来的，梯度不会通过 $u$ 传播到 $x$，因此我们只考虑 $z$ 关于 $x$ 的直接影响：$$\n",
    "     \\frac{dz}{dx} = \\frac{d}{dx}(u \\cdot x) = u\n",
    "     $$\n",
    "   - 这里 $u$ 等于 $y$，即 $y = x^2$，所以：\n",
    "     $$\n",
    "     \\frac{dz}{dx} = x^2\n",
    "     $$\n",
    "   - 当我们计算 $z = u \\cdot x$ 关于 $x$ 的偏导数时，如果我们将 $u$ 视为常量，那么梯度就是 $u$: $   \\frac{\\partial z}{\\partial x} = \\frac{\\partial (u \\cdot x)}{\\partial x} = u$\n",
    "      "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37aa62986c901085"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([0., 1., 4., 9.]), array([ True,  True,  True,  True]))"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x * x\n",
    "    u = y.detach()\n",
    "    z = u * x\n",
    "z.backward()\n",
    "x.grad, x.grad == u"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:37:39.779540Z",
     "start_time": "2024-06-14T06:37:39.714030Z"
    }
   },
   "id": "cee236de74e33daf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "由于记录了y的计算结果，我们可以随后在y上调用反向传播， 得到y=x*x关于的x的导数，即2*x。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e926ffedfc4654d2"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([0., 2., 4., 6.]), array([ True,  True,  True,  True]))"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()  # 在这个步骤中，MXNet 计算了 y = x*x 关于 x 的梯度\n",
    "x.grad, x.grad == 2 * x  # y对于x的梯度, 就是2x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:37:39.779622Z",
     "start_time": "2024-06-14T06:37:39.719476Z"
    }
   },
   "id": "9cd140fd80fd8d80"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5.4. Gradients and Python Control Flow Python控制流的梯度计算\n",
    "使用自动微分的一个好处是： 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。 在下面的代码中，while循环的迭代次数和if语句的结果都取决于输入a的值。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f616d3b3a0094463"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "array(102400.)"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while np.linalg.norm(b) < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "# 让我们计算梯度。\n",
    "a = np.random.normal()\n",
    "a.attach_grad()\n",
    "with autograd.record():\n",
    "    d = f(a)\n",
    "d.backward()\n",
    "a.grad "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:37:39.780791Z",
     "start_time": "2024-06-14T06:37:39.725606Z"
    }
   },
   "id": "b96760ff5fbb34e6"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:37:39.781168Z",
     "start_time": "2024-06-14T06:37:39.733377Z"
    }
   },
   "id": "c0b09a109c820755"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
