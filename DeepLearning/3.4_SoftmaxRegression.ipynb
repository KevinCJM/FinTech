{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4. Softmax 回归 Softmax Regression\n",
    "\n",
    "前几节介绍的线性回归模型适用于输出为连续值的情景。在另一类情景中，模型输出可以是一个像图像类别这样的离散值。对于这样的离散值预测问题，我们可以使用诸如softmax回归在内的分类模型。和线性回归不同，softmax回归的输出单元从一个变成了多个，且引入了softmax运算使输出更适合离散值的预测和训练。本节以softmax回归模型为例，介绍神经网络中的分类模型。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84088edac58406a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.1. 分类问题 Classification\n",
    "\n",
    "让我们考虑一个简单的图像分类问题，其输入图像的高和宽均为2像素，且色彩为灰度。这样每个像素值都可以用一个标量表示。我们将图像中的4像素分别记为 $x_1, x_2, x_3, x_4$。假设训练数据集中图像的真实标签为狗、猫或鸡（假设可以用4像素表示出这3种动物），这些标签分别对应离散值 $y_1, y_2, y_3$。\n",
    "\n",
    "我们通常使用离散的数值来表示类别，例如 $y_1=1, y_2=2, y_3=3$。如此，一张图像的标签为1、2和3这3个数值中的一个。虽然我们仍然可以使用回归模型来进行建模，并将预测值就近定点化到1、2和3这3个离散值之一，但这种连续值到离散值的转化通常会影响到分类质量。因此我们一般使用更加适合离散值输出的模型来解决分类问题。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1c7a173c9d2c091"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.2. Softmax回归模型 Softmax Regression\n",
    "\n",
    "softmax回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，softmax回归的输出值个数等于标签里的类别数。因为一共有4种特征和3种输出动物类别，所以权重包含12个标量（带下标的 $w$），偏差包含3个标量（带下标的 $b$），且对每个输入计算 $o_1, o_2, o_3$ 这3个输出：\n",
    "\n",
    "$$o_1 = \\sum_{i=1}^{4} w_i x_i + b_1$$\n",
    "$$o_2 = \\sum_{i=1}^{4} w_i x_i + b_2$$\n",
    "$$o_3 = \\sum_{i=1}^{4} w_i x_i + b_3$$\n",
    "\n",
    "这里关于权重和偏差的描述确实需要进一步阐明。对于softmax回归模型，每个输入特征的权重不仅依赖于输入特征的数量，还依赖于输出类别的数量。在这个例子中，有4种输入特征和3种输出类别。因此，权重的数量为 \\(4 \\times 3 = 12\\) 个。 让我们详细解释一下：\n",
    "- 权重和偏差的结构\n",
    "    - 在分类问题中，如softmax回归，每个输入特征 $x_i$ 对每个输出类别 $o_j$ 都有一个唯一的权重 $w_{ij}$。\n",
    "      这意味着：对于每个输出类别，每个输入特征都有一个对应的权重。如果有 $n$ 个特征和 $m$ 个输出类别，那么将有 $n \\times m$ 个权重。\n",
    "    - 在上面例子中，有4个特征和3个输出类别，所以权重矩阵 $W$ 的维度是 $4 \\times 3$（每列对应一个输出类别，每行对应一个输入特征），共有12个权重元素。\n",
    "- 具体的权重计算, 对于每个输出类别 \\(o_j\\) 的计算如下：\n",
    "  这里，$w_{ij}$ 表示第 $i$ 个特征对第 $j$ 个输出的贡献的权重。每个输出$o_j$都是所有输入$x_i$的线性组合，加上该输出类别特有的偏差$b_j$。\n",
    "$$o_1 = w_{11}x_1 + w_{21}x_2 + w_{31}x_3 + w_{41}x_4 + b_1$$\n",
    "$$o_2 = w_{12}x_1 + w_{22}x_2 + w_{32}x_3 + w_{42}x_4 + b_2$$\n",
    "$$o_3 = w_{13}x_1 + w_{23}x_2 + w_{33}x_3 + w_{43}x_4 + b_3$$\n",
    "\n",
    "下图用神经网络图描绘了上面的计算。softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出$0_1, o_2, o_3$的计算都要依赖于所有的输入$x_1, x_2, x_3, x_4$，softmax回归的输出层也是一个全连接层。\n",
    " ![](/Users/chenjunming/Desktop/FinTech/DeepLearning/pics/SoftmaxRegression.png)\n",
    " \n",
    "为了更简洁地表达模型，我们仍然使用线性代数符号。 通过向量形式表达为 $\\mathbf{o} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}$， 这是一种更适合数学和编写代码的形式。 由此，我们已经将所有权重放到一个 $4 \\times 3$ 的矩阵 $\\mathbf{W}$ 中，其中第 $i$ 行和第 $j$ 列的元素 $w_{ij}$ 是输入特征 $x_i$ 和输出类别 $j$ 的权重。偏置向量 $\\mathbf{b}$ 包含了3个偏置，其中 $b_j$ 是输出类别 $j$ 的偏置。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20240940a1afa66f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.3. 全连接层的参数开销\n",
    "\n",
    "正如我们将在后续章节中看到的，在深度学习中，全连接层无处不在。 然而，顾名思义，全连接层是“完全”连接的，可能有很多可学习的参数。 具体来说，对于任何具有 $p$ 个输入和 $q$ 个输出的全连接层， 参数开销为 $$\\mathcal{O}(dq)$ ，这个数字在实践中可能高得令人望而却步。 幸运的是，将$d$个输入转为$q$个输出的成本可以减少到$\\mathcal{O}(\\frac{dq}{n})$， 其中超参数$n$可以由我们灵活指定，以在实际应用中平衡参数节约和模型有效性。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "259b650fa09acd4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.4. softmax运算\n",
    "\n",
    "现在我们将优化参数以最大化观测数据的概率。 为了得到预测结果，我们将设置一个阈值，如选择具有最大概率的标签。\n",
    "\n",
    "我们希望模型的输出$\\hat{y}_j$可以视为属于类别$j$的概率， 然后选择具有最大输出值的类别$\\operatorname{argmax}_j y_j$作为我们的预测。 例如，如果$\\hat{y}_1, \\hat{y}_2, \\hat{y}_3$分别为0.1, 0.8, 0.1，那么我们预测类别2，在我们的例子中代表“鸡”。\n",
    "\n",
    "然而我们能否将未规范化的预测$o$直接视作我们感兴趣的输出呢？ 答案是否定的。 因为将线性层的输出直接视为概率时存在一些问题： 一方面，我们没有限制这些输出数字的总和为1。 另一方面，根据输入的不同，它们可以为负值。 这些违反了 2.6节中所说的概率基本公理。\n",
    "\n",
    "要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。 此外，我们需要一个训练的目标函数，来激励模型精准地估计概率。 例如， 在分类器输出0.5的所有样本中，我们希望这些样本是刚好有一半实际上属于预测的类别。 这个属性叫做校准（calibration）。\n",
    "\n",
    "社会科学家邓肯·卢斯于1959年在选择模型（choice model）的理论基础上 发明的softmax函数正是这样做的： softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。 为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式：\n",
    "$$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o})\\quad \\text{其中}\\quad \\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}$$\n",
    "其中:\n",
    "- $\\mathbf{o}$：是模型的原始输出，也称为logits，可以是任意实数。\n",
    "- $\\exp(o_j)$：对每个输出 $o_j$ 应用指数函数，确保非负性，因为指数函数总是返回正值。\n",
    "- $\\sum_k \\exp(o_k)$：是对所有指数化输出的求和，这个求和作为归一化的分母，确保所有输出值的总和为1。\n",
    "\n",
    "这里，对于所有的 $j$ 总有$0\\leq \\hat{y}_j \\leq 1$，并且 $\\sum_j \\hat{y}_j = 1$。 因此，$\\hat{\\mathbf{y}}$ 是一个合法的概率分布。 这个要求是我们在分类问题中需要的。 softmax运算不会改变未规范化的预测$o$的相对大小和顺序，只会确定分配给每个类别的概率。此外, Softmax函数是可微的，这意味着我们可以计算其导数来进行梯度下降等优化方法。因此，在预测过程中，我们仍然可以用下式来选择最有可能的类别。\n",
    "$$\\operatorname{argmax}_j \\hat y_j = \\operatorname*{argmax}_j o_j$$\n",
    "\n",
    "尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个线性模型（linear model）。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c02bf4f68504db7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "重新表达一下。\n",
    "\n",
    "Softmax回归在很多方面与线性回归相似，尤其是在模型的输出计算方式上，都涉及到输入特征和权重的线性组合。不同之处主要在于Softmax回归在计算完线性输出后，还会进一步通过Softmax函数转换这些输出，使其成为合法的概率分布。Softmax回归可以分为以下几步：1) 线性回归计算输出；2) Softmax函数转换输出。\n",
    "\n",
    "**线性输出阶段**\n",
    "\n",
    "在Softmax回归的线性阶段，每个输出 $o_j$ 是输入特征向量 $\\mathbf{x}$ 与相应的权重向量 $\\mathbf{w}_j$ 的点积加上偏置 $b_j$。具体来说，对于三个类别的情况，模型的输出计算如下：\n",
    "\n",
    "- $o_1 = w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + w_{14}x_4 + b_1$\n",
    "- $o_2 = w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + w_{24}x_4 + b_2$\n",
    "- $o_3 = w_{31}x_1 + w_{32}x_2 + w_{33}x_3 + w_{34}x_4 + b_3$\n",
    "\n",
    "其中，$w_{ij}$ 表示针对第 $j$ 类别的第 $i$ 个特征的权重，而 $b_j$ 是对应于第 $j$ 类别的偏置。\n",
    "\n",
    "**Softmax转换阶段**\n",
    "\n",
    "计算完这些线性输出之后，Softmax回归将使用Softmax函数对每个输出进行转换。这个转换的目的是将未规范化的线性输出转换为有效的概率分布。这是通过以下步骤完成的：\n",
    "\n",
    "1. **指数化**：首先对每个输出 \\(o_j\\) 应用指数函数，确保结果是非负的。这一步骤生成 \\( \\exp(o_j) \\)。\n",
    "   \n",
    "2. **归一化**：然后，将每个指数化的结果除以所有类别指数化结果的总和。这确保了所有输出值的和为1，形成了一个合法的概率分布：\n",
    "    其中，$\\hat{y}_j$ 是属于第 $j$ 类的预测概率。\n",
    "\n",
    "   $$\n",
    "   \\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_{k=1}^3 \\exp(o_k)}\n",
    "   $$\n",
    "\n",
    "通过这种方式，Softmax回归不仅能进行多类分类，还能提供关于分类决策的概率解释，这在很多应用场景中是非常宝贵的信息。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c8127349662d6eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.5. 小批量样本的矢量化 Vectorization\n",
    "\n",
    "为了提高计算效率并且充分利用GPU，我们通常会对小批量样本的数据执行矢量计算。 假设我们读取了一个批量的样本 $\\mathbf{X}$，其中特征维度（输入数量）为 $d$, 批量大小为 $n$。此外，假设我们有 $q$ 个类别。 那么小批量样本的特征为 $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，权重为 $\\mathbf{W} \\in \\mathbb{R}^{d \\times q}$，偏置为 $\\mathbf{b} \\in \\mathbb{R}^{1 \\times q}$。softmax回归的矢量计算表达式为：\n",
    "$$\\mathbf{O} = \\mathbf{X} \\mathbf{W} + \\mathbf{b}$$\n",
    "$$\\hat{\\mathbf{Y}} = \\mathrm{softmax}(\\mathbf{O})$$\n",
    "\n",
    "相对于一次处理一个样本， 小批量样本的矢量化加快了 $\\mathbf{X}$ 和 $\\mathbf{W}$ 的矩阵-向量乘法运算。 由于 $\\mathbf{X}$ 中的每一行代表一个数据样本， 那么softmax运算可以按行（rowwise）执行： 对于 $\\mathbf{O}$ 的每一行，我们先对所有项进行幂运算，然后通过求和对它们进行标准化。 $\\mathbf{X}\\mathbf{Y}+\\mathbf{b}$ 的求和会使用广播机制， 小批量的未规范化预测$\\mathbf{O}$和输出概率$\\hat{\\mathbf{Y}}$的形状是 $\\mathbb{R}^{n \\times q}$。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed3d54afc76cd7cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.6. 损失函数\n",
    "\n",
    "接下来，我们需要一个损失函数来度量预测的效果。 我们将使用最大似然估计，这与在线性回归 （ 3.1.3节） 中的方法相同。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4b757464e109aee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
