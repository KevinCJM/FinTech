{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 手动实现实现奇异值分解(SVD)用于计算伪逆矩阵\n",
    "\n",
    "在常用的机器学习库 scikit-learn 中, 我们经常会使用 LinearRegression 来进行线性回归分析, 其函数内部使用的是 scipy.linalg.lstsq 函数，该函数使用了最小二乘法求解线性方程组。具体来说，它采用了 LAPACK 库中的 dgelsd 例程，该例程基于奇异值分解（SVD, Singular Value Decomposition），这是一个数值稳定的方法，适用于求解即使是奇异或接近奇异的矩阵。利用SVD，可以稳定地求解线性回归的解析解，即使矩阵是奇异的。\n",
    "在这里，我们将手动实现奇异值分解(SVD)用于计算伪逆矩阵，以此来求解线性回归的解析解。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8521b18670e46214"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 奇异值分解 (SVD)\n",
    "\n",
    "奇异值分解 (SVD) 将一个矩阵分解为三个矩阵的乘积：$\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T$\n",
    "其中：\n",
    "- $\\mathbf{A}$ 是一个 $m \\times n$ 的矩阵。\n",
    "- $\\mathbf{U}$ 是一个 $m \\times m$ 的正交矩阵。\n",
    "- $\\mathbf{\\Sigma}$ 是一个 $m \\times n$ 的对角矩阵，其对角线上的元素是矩阵 $\\mathbf{A}$ 的奇异值。\n",
    "- $\\mathbf{V}^T$ 是一个 $n \\times n$ 的正交矩阵的转置。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebf8a35aefd3abc1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.0 SVD 计算示例\n",
    "假设我们有一个 $3 \\times 2$ 的矩阵：$\\mathbf{A} = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{pmatrix}$\n",
    "\n",
    "为了详细求解矩阵 $\\mathbf{A} = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{pmatrix}$ 的奇异值分解 (SVD)，即求 $\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T$，我们需要执行以下步骤：\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0add94947e967e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 步骤1：计算 $\\mathbf{A}^T \\mathbf{A}$ 和 $\\mathbf{A} \\mathbf{A}^T$\n",
    "对于任何矩阵 $\\mathbf{A}$，无论它是否是方阵，$\\mathbf{A}^T \\mathbf{A}$ 和 $\\mathbf{A} \\mathbf{A}^T$ 都一定是方阵。\n",
    "这两个乘积结果方阵的尺寸分别取决于原矩阵的行数和列数。这种性质在许多数学和工程应用中非常有用，特别是在涉及到求解最小二乘问题和进行矩阵分解时。\n",
    "$$\n",
    "\\mathbf{A}^T = \\begin{pmatrix} 1 & 3 & 5 \\\\ 2 & 4 & 6 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^T \\mathbf{A} = \\begin{pmatrix} 1 & 3 & 5 \\\\ 2 & 4 & 6 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{pmatrix} = \\begin{pmatrix} 35 & 44 \\\\ 44 & 56 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\mathbf{A}^T = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{pmatrix} \\begin{pmatrix} 1 & 3 & 5 \\\\ 2 & 4 & 6 \\end{pmatrix} = \\begin{pmatrix} 5 & 11 & 17 \\\\ 11 & 25 & 39 \\\\ 17 & 39 & 61 \\end{pmatrix}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12e6a6aec9736b38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 步骤2：计算 $\\mathbf{A}^T \\mathbf{A}$ 的特征值和特征向量\n",
    "\n",
    "特征值方程为：\n",
    "$$\n",
    "\\det(\\mathbf{A}^T \\mathbf{A} - \\lambda \\mathbf{I}) = 0\n",
    "$$\n",
    "- 这里使用了行列式, 行列式$det()$是一个定义在方阵上的数学函数，其值可以从方阵中的元素通过特定的代数方法计算得到。行列式是方阵属性的核心概念之一，具有多种重要的数学和实际应用:\n",
    "    - 解线性方程组：行列式用于判断线性方程组（表示为矩阵形式 $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$）是否有唯一解。当 $\\text{det}(\\mathbf{A}) \\neq 0$ 时，方程组有唯一解。\n",
    "    - 矩阵可逆性：行列式的值可以用来判断一个方阵是否可逆。具体来说，当且仅当 $\\text{det}(\\mathbf{A}) \\neq 0$ 时，矩阵 $\\mathbf{A}$ 是可逆的。\n",
    "    - 几何解释：在几何学中，行列式可以解释为线性变换后形成的新图形与原图形的体积（或面积、长度）比例。例如，二维空间中一个矩阵的行列式表示对应线性变换下的面积扩大或缩小的倍数。\n",
    "- 对于二阶和三阶行列式, 可以直接通过特定公式计算。例如，对于二阶行列式：$\n",
    "  \\text{det}\\left(\\begin{array}{cc}\n",
    "  a & b \\\\\n",
    "  c & d\n",
    "  \\end{array}\\right) = ad - bc $\n",
    "    - 对于更高阶的行列式，可以通过拉普拉斯展开沿某一行或某一列展开计算行列式。\n",
    "- 这里, 特征值方程为：$\\det(\\mathbf{A}^T \\mathbf{A} - \\lambda \\mathbf{I}) = 0$, 是为了找到使方阵 $\\mathbf{A}^T \\mathbf{A} - \\lambda \\mathbf{I}$ 成为奇异矩阵（即不可逆矩阵）的 $\\lambda$值。\n",
    "- 在代码中, 我们可以通过下面代码来实现行列式的计算：\n",
    "```python\n",
    "import numpy as np\n",
    "from numba import njit, float64\n",
    "\n",
    "@njit(float64(float64[:, :]))\n",
    "def det_numba(matrix):\n",
    "    \"\"\"\n",
    "    计算矩阵的行列式值。\n",
    "\n",
    "    该函数使用Numba的Just-In-Time编译器优化，以提升数学运算的执行速度。\n",
    "    它接受一个二维浮点数数组作为输入，并返回该矩阵的行列式值。\n",
    "\n",
    "    参数:\n",
    "    matrix: 二维浮点数数组，表示需要计算行列式的矩阵。\n",
    "\n",
    "    返回值:\n",
    "    浮点数，表示输入矩阵的行列式值。\n",
    "    \"\"\"\n",
    "    # 使用NumPy的线性代数函数计算矩阵的行列式\n",
    "    return np.linalg.det(matrix)\n",
    "```\n",
    "\n",
    "\n",
    "对于矩阵 $\\mathbf{A}^T \\mathbf{A}$：\n",
    "$$\n",
    "\\mathbf{A}^T \\mathbf{A} = \\begin{pmatrix} 35 & 44 \\\\ 44 & 56 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "其特征值方程为：\n",
    "$$\n",
    "\\det\\begin{pmatrix} 35 - \\lambda & 44 \\\\ 44 & 56 - \\lambda \\end{pmatrix} = (35 - \\lambda)(56 - \\lambda) - 44^2 = 0\n",
    "$$\n",
    "\n",
    "展开并整理得到：\n",
    "$$\n",
    "35\\times56 - 35\\lambda - 56\\lambda + \\lambda^2 - 44^2 = 0\n",
    "$$\n",
    "$$\n",
    "\\lambda^2 - 91\\lambda + 1960 - 1936 = 0\n",
    "$$\n",
    "$$\n",
    "\\lambda^2 - 91\\lambda + 24 = 0\n",
    "$$\n",
    "\n",
    "解这个二次方程：\n",
    "- 对于一般形式的二次方程：$a\\lambda^2 + b\\lambda + c = 0$, 其求解公式为: $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$\n",
    "- 因此:\n",
    "$$\n",
    "\\lambda = \\frac{91 \\pm \\sqrt{91^2 - 4 \\cdot 24}}{2}\n",
    "$$\n",
    "$$\n",
    "\\lambda = \\frac{91 \\pm \\sqrt{8281}}{2}\n",
    "$$\n",
    "$$\n",
    "\\lambda = \\frac{91 \\pm 90.9824}{2}\n",
    "$$\n",
    "\n",
    "计算得到两个特征值：\n",
    "$$\n",
    "\\lambda_1 = \\frac{91 + 90.9824}{2} = \\frac{91+90.47098983}{2} = 90.73549491\n",
    "$$\n",
    "$$\n",
    "\\lambda_2 = \\frac{91-90.47098983}{2} = \\frac{0.0176}{2} = 0.264505087\n",
    "$$\n",
    "\n",
    "- 在python中, 我们可以通过下面代码来实现行列式的计算：\n",
    "```python\n",
    "import numpy as np\n",
    "from numba import njit, float64\n",
    "\n",
    "@njit(float64[:](float64[:, :]))\n",
    "def compute_eigen_values(matrix):\n",
    "    \"\"\"\n",
    "    计算给定矩阵的特征值。\n",
    "\n",
    "    参数:\n",
    "    matrix: float64[:, :] 类型的二维数组，表示输入的矩阵, 需要是方阵。\n",
    "\n",
    "    返回值:\n",
    "    float64[:] 类型的一维数组，包含输入矩阵的特征值。\n",
    "    \"\"\"\n",
    "    # 使用numpy的linalg.eigvals函数计算矩阵的特征值\n",
    "    eigen_values = np.linalg.eigvals(matrix)\n",
    "    # 由于奇异值矩阵中奇异值的顺序是按降序排列的, 所以需要进行排序操作\n",
    "    eigen_values[:] = np.sort(eigen_values)[::-1]  # 从大到小排序, 避免分配新的内存空间\n",
    "    # 返回计算得到的特征值数组\n",
    "    return eigen_values\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e15c36ffa6982a55"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 步骤3：计算奇异值$\\mathbf{\\Sigma}$\n",
    "\n",
    "我们已经得到了 $\\mathbf{A}^T \\mathbf{A}$ 的特征值：$\\lambda_1 = 90.73549491$ 和 $\\lambda_2 = 0.264505087$\n",
    "\n",
    "奇异值是这些特征值的非负平方根：\n",
    "$$\\sigma_1 = \\sqrt{\\lambda_1} = \\sqrt{90.73549491} \\approx 9.524$$\n",
    "$$\\sigma_2 = \\sqrt{\\lambda_2} = \\sqrt{0.264505087} \\approx 0.514$$\n",
    "\n",
    "因此，$\\mathbf{\\Sigma}$ 矩阵为：\n",
    "$$\n",
    "\\mathbf{\\Sigma} = \\begin{pmatrix}\n",
    "9.524 & 0 \\\\\n",
    "0 & 0.514 \\\\\n",
    "0 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- 在python中, 我们可以通过下面代码来实现行列式的计算：\n",
    "```python\n",
    "@njit(float64[:, :](float64[:], int64, int64))\n",
    "def create_sigma_matrix(eigen_values, m, n):\n",
    "    \"\"\"\n",
    "    根据给定的特征值数组，创建一个奇异值矩阵。\n",
    "\n",
    "    参数:\n",
    "    eigen_values: 一维数组，包含特征值。\n",
    "    m: 整数，指定奇异值矩阵的行数。\n",
    "    n: 整数，指定奇异值矩阵的列数。\n",
    "\n",
    "    返回:\n",
    "    一个二维数组，为生成的奇异值矩阵。\n",
    "    \"\"\"\n",
    "    # 计算特征值的平方根，得到奇异值\n",
    "    singular_values = np.sqrt(eigen_values)\n",
    "    # 初始化一个大小为(m, n)的零矩阵, 初值为0\n",
    "    the_sigma_matrix = np.zeros((m, n))\n",
    "    # 将计算得到的奇异值填充到矩阵的对角线上\n",
    "    np.fill_diagonal(the_sigma_matrix, singular_values[:min(m, n)])\n",
    "    return the_sigma_matrix\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73e769a391df2008"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 步骤4：计算 $\\mathbf{V}$ 矩阵\n",
    "\n",
    "$\\mathbf{V}$ 矩阵是 $\\mathbf{A}^T \\mathbf{A}$ 的特征向量矩阵。我们已经知道$\\mathbf{A}^T \\mathbf{A} = \\begin{pmatrix} 35 & 44 \\\\ 44 & 56 \\end{pmatrix}$的特征值为 $\\lambda_1 = 90.73549491$, $\\lambda_2 = 0.26450509$; 接下来就是找到特征向量 (Eigenvector)\n",
    "- 特征向量是与特征值相关的向量。对于一个方阵 $\\mathbf{A}$，如果存在一个非零向量 $\\mathbf{v}$ 和一个标量 $\\lambda$ 使得 $\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}$, 那么 $\\mathbf{v}$ 就被称为矩阵 $\\mathbf{A}$ 对应于特征值 $\\lambda$ 的特征向量，$\\lambda$ 被称为特征值。\n",
    "\n",
    "- 在上述方程$\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}$中，矩阵 $\\mathbf{A}$ 作用在向量 $\\mathbf{v}$ 上时，结果仍然是向量 $\\mathbf{v}$ 但被缩放了一个比例 $\\lambda$。换句话说，矩阵 $\\mathbf{A}$ 只改变了向量 $\\mathbf{v}$ 的长度，而没有改变其方向。\n",
    "\n",
    "- 基于$\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}$可以推导出$(\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{v} = 0$, 将方阵A和特征值$\\lambda$带入, 可以求出特征向量$\\mathbf{v}$。\n",
    "\n",
    "- 一般求解时, 会先假设一个特征向量的某个值为1，然后求解其他值，再进行L2范数的归一化处理。\n",
    "\n",
    "- 对于每个特征值$\\lambda$，都有一个对应的特征向量$\\mathbf{v}$。如果有多个特征值, 可以构建出特征矩阵$\\mathbf{V}$, 其中每一列是一个特征向量。\n",
    "\n",
    "\n",
    "对于 $\\lambda_1 = 90.73549491$，我们解以下方程：\n",
    "$$\n",
    "\\begin{pmatrix} 35 - 90.73549491 & 44 \\\\ 44 & 56 - 90.73549491 \\end{pmatrix} \\begin{pmatrix} v_{11} \\\\ v_{21} \\end{pmatrix} = \\begin{pmatrix} -55.73549491 & 44 \\\\ 44 & -34.73549491 \\end{pmatrix} \\begin{pmatrix} v_{11} \\\\ v_{21} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\begin{pmatrix} -55.73549491 & 44 \\\\ 44 & -34.73549491 \\end{pmatrix} \\begin{pmatrix} v_{11} \\\\ v_{21} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "我们假设 $v_{11} = 1$，然后解得：\n",
    "$$\n",
    "-55.73549491 \\cdot 1 + 44 \\cdot v_{21} = 0 \\implies v_{21} = \\frac{55.73549491}{44} = 1.266\n",
    "$$\n",
    "\n",
    "解得特征向量 $\\mathbf{v}_1$：\n",
    "$$\n",
    "\\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 1.266 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "归一化特征向量：\n",
    "$$\n",
    "\\mathbf{v}_1 = \\frac{1}{\\sqrt{1^2 + 1.266^2}} \\begin{pmatrix} 1 \\\\ 1.266 \\end{pmatrix} = \\frac{1}{1.598} \\begin{pmatrix} 1 \\\\ 1.266 \\end{pmatrix} = \\begin{pmatrix} 0.626 \\\\ 0.793 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "所以 $\\mathbf{v}_1 = \\begin{pmatrix} 0.626 \\\\ 0.793 \\end{pmatrix}$。\n",
    "\n",
    "对于 $\\lambda_2 = 0.264505087$，我们解以下方程：\n",
    "$$\n",
    "\\begin{pmatrix} 35 - 0.26450509 & 44 \\\\ 44 & 56 - 0.26450509 \\end{pmatrix} \\begin{pmatrix} v_{12} \\\\ v_{22} \\end{pmatrix} = \\begin{pmatrix} 34.73549491 & 44 \\\\ 44 & 55.73549491 \\end{pmatrix} \\begin{pmatrix} v_{12} \\\\ v_{22} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\begin{pmatrix} 34.73549491 & 44 \\\\ 44 & 55.73549491 \\end{pmatrix} \\begin{pmatrix} v_{12} \\\\ v_{22} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "我们假设 $v_{12} = 1$，然后解得：\n",
    "$$\n",
    "34.73549491 \\cdot 1 + 44 \\cdot v_{22} = 0 \\implies v_{22} = -\\frac{34.73549491}{44} = -0.79\n",
    "$$\n",
    "\n",
    "解得特征向量 $\\mathbf{v}_2$：\n",
    "$$\n",
    "\\mathbf{v}_2 = \\begin{pmatrix} 1 \\\\ -0.79 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "归一化特征向量：\n",
    "$$\n",
    "\\mathbf{v}_2 = \\frac{1}{\\sqrt{1^2 + (-0.79)^2}} \\begin{pmatrix} 1 \\\\ -0.79 \\end{pmatrix} = \\frac{1}{1.265} \\begin{pmatrix} 1 \\\\ -0.79 \\end{pmatrix} = \\begin{pmatrix} 0.79 \\\\ -0.625 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "所以 $\\mathbf{v}_2 = \\begin{pmatrix} 0.79 \\\\ -0.625 \\end{pmatrix}$。\n",
    "\n",
    "最终求得特征矩阵为:\n",
    "$$\n",
    "\\mathbf{V} = \\begin{pmatrix} 0.626 & 0.793 \\\\ 0.79 & -0.625 \\end{pmatrix}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5a58b559d1fc9ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5 步骤5：计算 $\\mathbf{U}$ 矩阵\n",
    "\n",
    "$\\mathbf{U}$ 矩阵是 $\\mathbf{A} \\mathbf{A}^T$ 的特征向量矩阵。我们计算出$\\mathbf{A} \\mathbf{A}^T = \\begin{pmatrix} 5 & 11 & 17 \\\\ 11 & 25 & 39 \\\\ 17 & 39 & 61 \\end{pmatrix}$\n",
    "重复上面的计算过程, 可以求得$\\mathbf{U}$ 矩阵。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cf6126df68e6374"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.6 代码实现\n",
    "\n",
    "其实说了那么多, 我们可以通过 numpy.linalg 的 svd 函数来实现奇异值分解(SVD)。下面是一个简单的示例代码：\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# 创建一个矩阵\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "U, S, VT = np.linalg.svd(A)\n",
    "print(\"U:\", U)\n",
    "'''\n",
    "[[-0.2298477   0.88346102  0.40824829]\n",
    " [-0.52474482  0.24078249 -0.81649658]\n",
    " [-0.81964194 -0.40189603  0.40824829]]\n",
    "'''\n",
    "\n",
    "print(\"S:\", S)\n",
    "'''\n",
    "[9.52551809 0.51430058]\n",
    "'''\n",
    "\n",
    "print(\"VT:\", VT)\n",
    "'''\n",
    "[[-0.61962948 -0.78489445]\n",
    " [-0.78489445  0.61962948]]\n",
    "'''\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdafdd53227bfb69"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 伪逆矩阵\n",
    "\n",
    "伪逆矩阵（Pseudoinverse）是对矩阵求逆的一种广义形式。对于一个给定的矩阵 $\\mathbf{A}$，其伪逆矩阵通常记作 $\\mathbf{A}^+$。伪逆矩阵在某些情况下代替了逆矩阵，特别是在矩阵不可逆或不是方阵的情况下。\n",
    "伪逆矩阵有多种定义，其中最常见的一种是使用奇异值分解（SVD）来计算的。\n",
    "\n",
    "- Moore-Penrose 伪逆是最常用的一种伪逆，满足以下四个条件：\n",
    "    1. $\\mathbf{A} \\mathbf{A}^+ \\mathbf{A} = \\mathbf{A}$\n",
    "    2. $\\mathbf{A}^+ \\mathbf{A} \\mathbf{A}^+ = \\mathbf{A}^+$\n",
    "    3. $(\\mathbf{A} \\mathbf{A}^+)^T = \\mathbf{A} \\mathbf{A}^+$\n",
    "    4. $(\\mathbf{A}^+ \\mathbf{A})^T = \\mathbf{A}^+ \\mathbf{A}$\n",
    "- 伪逆矩阵（Moore-Penrose Pseudoinverse）$\\mathbf{A}^+$ 的计算公式为：$\\mathbf{A}^+ = \\mathbf{V} \\mathbf{\\Sigma}^+ \\mathbf{U}^T$\n",
    "    - 其中，$\\mathbf{\\Sigma}^+$ 是通过将 $\\mathbf{\\Sigma}$ 的非零奇异值取倒数，并在其他位置补零得到的。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1faead843097747b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 计算奇异值矩阵的伪逆 $\\mathbf{\\Sigma}^+$\n",
    "\n",
    "在第一部的奇异值分解中, 我们已经计算得到：\n",
    "$$\n",
    "\\mathbf{\\Sigma} = \\begin{pmatrix}\n",
    "9.52551809 & 0 \\\\\n",
    "0 & 0.51430058 \\\\\n",
    "0 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "伪逆矩阵 $\\mathbf{\\Sigma}^+$ 是通过取倒数非零奇异值并转置得到的：\n",
    "$$\n",
    "\\mathbf{\\Sigma}^+ = \\begin{pmatrix}\n",
    "\\frac{1}{9.52551809} & 0 & 0 \\\\\n",
    "0 & \\frac{1}{0.51430058} & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "计算得到：\n",
    "$$\n",
    "\\mathbf{\\Sigma}^+ = \\begin{pmatrix}\n",
    "0.10497737 & 0 & 0 \\\\\n",
    "0 & 1.94438824 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26812ea54422fb5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 计算伪逆矩阵 $\\mathbf{A}^+$\n",
    "\n",
    "根据伪逆矩阵的定义：\n",
    "$$\n",
    "\\mathbf{A}^+ = \\mathbf{V} \\mathbf{\\Sigma}^+ \\mathbf{U}^T\n",
    "$$\n",
    "\n",
    "我们已经有：\n",
    "$$\n",
    "\\mathbf{V} = \\begin{pmatrix}\n",
    "-0.61962948 & -0.78489445 \\\\\n",
    "-0.78489445 & 0.61962948\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{U} = \\begin{pmatrix}\n",
    "-0.2298477 & 0.88346102 & 0.40824829 \\\\\n",
    "-0.52474482 & 0.24078249 & -0.81649658 \\\\\n",
    "-0.81964194 & -0.40189603 & 0.40824829\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{\\Sigma}^+ = \\begin{pmatrix}\n",
    "0.10497737 & 0 & 0 \\\\\n",
    "0 & 1.94438824 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "首先计算 $\\mathbf{V} \\mathbf{\\Sigma}^+$：\n",
    "$$\n",
    "\\mathbf{V} \\mathbf{\\Sigma}^+ = \\begin{pmatrix}\n",
    "-0.61962948 & -0.78489445 \\\\\n",
    "-0.78489445 & 0.61962948\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "0.10497737 & 0 & 0 \\\\\n",
    "0 & 1.94438824 & 0\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "-0.06507094 & -1.52597303 & 0 \\\\\n",
    "-0.15300144 & 1.20450816 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "然后计算 $\\mathbf{A}^+$：\n",
    "$$\n",
    "\\mathbf{A}^+ = \\begin{pmatrix}\n",
    "-0.06507094 & -1.52597303 \\\\\n",
    "-0.15300144 & 1.20450816\n",
    "\\end{pmatrix} \\mathbf{U}^T\n",
    "$$\n",
    "$$\n",
    "\\mathbf{U}^T = \\begin{pmatrix}\n",
    "-0.2298477 & -0.52474482 & -0.81964194 \\\\\n",
    "0.88346102 & 0.24078249 & -0.40189603 \\\\\n",
    "0.40824829 & -0.81649658 & 0.40824829\n",
    "\\end{pmatrix}^T = \\begin{pmatrix}\n",
    "-0.2298477 & 0.88346102 & 0.40824829 \\\\\n",
    "-0.52474482 & 0.24078249 & -0.81649658 \\\\\n",
    "-0.81964194 & -0.40189603 & 0.40824829\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{A}^+ = \\begin{pmatrix}\n",
    "-0.06507094 & -1.52597303 \\\\\n",
    "-0.15300144 & 1.20450816\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "-0.2298477 & 0.88346102 & 0.40824829 \\\\\n",
    "-0.52474482 & 0.24078249 & -0.81649658 \\\\\n",
    "-0.81964194 & -0.40189603 & 0.40824829\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{A}^+ = \\begin{pmatrix}\n",
    "-0.96551724 & -0.03448276 & 0.89655172 \\\\\n",
    "0.76428571 & 0.02857143 & -0.71428571\n",
    "\\end{pmatrix}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2e6bc4fea67c88"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "eb569448165f0ba3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
